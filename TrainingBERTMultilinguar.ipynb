{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1NQSvvi_LI41KSQBnSRLWOPX5D-pxBiTw",
      "authorship_tag": "ABX9TyPD5rlXd1cYWOgtokfl1gAr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daksh024/NSP/blob/master/TrainingBERTMultilinguar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVh5XnLbVN0y",
        "outputId": "3f27ef20-51c9-491b-a692-3f71c68ed2be"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRmApF3bUYyd",
        "outputId": "d3d81fb3-6805-48c5-eeef-ce5a2c789071"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForMaskedLM.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load and tokenize text corpus from a file\n",
        "corpus_file_path = \"/content/drive/MyDrive/tinyCorpus.txt\"\n",
        "with open(corpus_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n"
      ],
      "metadata": {
        "id": "lFiIe1TJWXnT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = []\n",
        "for line in lines:\n",
        "    tokens = tokenizer.tokenize(line)\n",
        "    for i in range(len(tokens) - 1):\n",
        "        data.append((tokens[i], tokens[i+1]))\n"
      ],
      "metadata": {
        "id": "d969KtY2WaqT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define custom dataset\n",
        "class NextWordDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_token, target_token = self.data[idx]\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(input_token)\n",
        "        target_token_id = self.tokenizer.convert_tokens_to_ids(target_token)\n",
        "        return torch.tensor(input_ids), torch.tensor(target_token_id)\n",
        "\n",
        "# Create DataLoader for the dataset\n",
        "train_dataset = NextWordDataset(data, tokenizer)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXx2u1PcWdRi",
        "outputId": "71d7a25e-d80c-471c-f09f-0d57359e803c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Fine-tuning loop\n",
        "# num_epochs = 10\n",
        "# for batch in train_dataloader:\n",
        "#     optimizer.zero_grad()\n",
        "#     input_ids, target_ids = batch\n",
        "\n",
        "#     # Ensure input_ids is a 2D tensor\n",
        "#     input_ids = input_ids.unsqueeze(0) if input_ids.dim() == 1 else input_ids\n",
        "\n",
        "#     outputs = model(input_ids)[0]\n",
        "#     loss = loss_fn(outputs.view(-1, outputs.shape[-1]), target_ids)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "num_epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        # Start the timer\n",
        "        start_time = time.time()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, target_ids = batch\n",
        "\n",
        "        # Move data to GPU\n",
        "        input_ids = input_ids.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "\n",
        "        # print(input_ids)\n",
        "\n",
        "        # Ensure input_ids is a 2D tensor\n",
        "        input_ids = input_ids.unsqueeze(0) if input_ids.dim() == 1 else input_ids\n",
        "\n",
        "        outputs = model(input_ids)[0]\n",
        "\n",
        "        # Flatten both outputs and targets\n",
        "        outputs_flat = outputs.view(-1, outputs.shape[-1])\n",
        "        target_ids_flat = target_ids.view(-1)\n",
        "\n",
        "        loss = loss_fn(outputs_flat, target_ids_flat)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Stop the timer\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Calculate the elapsed time\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        print(f\"batch took {elapsed_time:.6f} seconds\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VU8UJH2vWgNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caab9c16-311a-4267-c8ad-7bfeb0ec9149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch took 6.077526 seconds\n",
            "batch took 3.356697 seconds\n",
            "batch took 2.659770 seconds\n",
            "batch took 3.054612 seconds\n",
            "batch took 3.473520 seconds\n",
            "batch took 3.271823 seconds\n",
            "batch took 2.693155 seconds\n",
            "batch took 3.012774 seconds\n",
            "batch took 2.855958 seconds\n",
            "batch took 2.985354 seconds\n",
            "batch took 3.484536 seconds\n",
            "batch took 3.865864 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader)"
      ],
      "metadata": {
        "id": "GAYkplgWn2bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model\n",
        "fine_tuned_model = BertForMaskedLM.from_pretrained(\"fine_tuned_bert\")\n",
        "\n"
      ],
      "metadata": {
        "id": "wGwuF85jWjiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "input_text = \"मैं\"\n",
        "input_ids = tokenizer.encode(input_text, add_special_tokens=True)\n",
        "with torch.no_grad():\n",
        "    outputs = fine_tuned_model(torch.tensor(input_ids).unsqueeze(0))\n",
        "    predicted_token_id = torch.argmax(outputs[0, -1]).item()\n",
        "    predicted_word = tokenizer.convert_ids_to_tokens(predicted_token_id)\n",
        "\n",
        "print(\"Predicted next word:\", predicted_word)"
      ],
      "metadata": {
        "id": "_xZE3tGmWkqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WcDPOyfNp2Fj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}