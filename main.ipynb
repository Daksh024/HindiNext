{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1LfZCZKKZFaxbcgxIevDAg2NpirXCbgT1",
      "authorship_tag": "ABX9TyNjbS7FIyCE2y1yt0Q2aXqn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daksh024/NSP/blob/Colab/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVh5XnLbVN0y",
        "outputId": "84098a24-9323-4c8a-a27b-d664dc0a612b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRmApF3bUYyd",
        "outputId": "4536d6d5-7df3-485c-a444-494e0e46f1bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForMaskedLM.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load and tokenize text corpus from a file\n",
        "corpus_file_path = \"/content/drive/MyDrive/tinyCorpus.txt\"\n",
        "with open(corpus_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n"
      ],
      "metadata": {
        "id": "lFiIe1TJWXnT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = []\n",
        "for line in lines:\n",
        "    tokens = tokenizer.tokenize(line)\n",
        "    for i in range(len(tokens) - 1):\n",
        "        data.append((tokens[i], tokens[i+1]))\n"
      ],
      "metadata": {
        "id": "d969KtY2WaqT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define custom dataset\n",
        "class NextWordDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_token, target_token = self.data[idx]\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(input_token)\n",
        "        target_token_id = self.tokenizer.convert_tokens_to_ids(target_token)\n",
        "        return torch.tensor(input_ids), torch.tensor(target_token_id)\n",
        "\n",
        "# Create DataLoader for the dataset\n",
        "train_dataset = NextWordDataset(data, tokenizer)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXx2u1PcWdRi",
        "outputId": "15f6c62f-e09c-432f-d4fe-4692b2383d98"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Fine-tuning loop\n",
        "# num_epochs = 10\n",
        "# for batch in train_dataloader:\n",
        "#     optimizer.zero_grad()\n",
        "#     input_ids, target_ids = batch\n",
        "\n",
        "#     # Ensure input_ids is a 2D tensor\n",
        "#     input_ids = input_ids.unsqueeze(0) if input_ids.dim() == 1 else input_ids\n",
        "\n",
        "#     outputs = model(input_ids)[0]\n",
        "#     loss = loss_fn(outputs.view(-1, outputs.shape[-1]), target_ids)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "\n",
        "import time\n",
        "\n",
        "num_epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, target_ids = batch\n",
        "\n",
        "        # Move data to GPU\n",
        "        input_ids = input_ids.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "\n",
        "        # print(input_ids)\n",
        "\n",
        "        # Ensure input_ids is a 2D tensor\n",
        "        input_ids = input_ids.unsqueeze(0) if input_ids.dim() == 1 else input_ids\n",
        "\n",
        "        outputs = model(input_ids)[0]\n",
        "\n",
        "        # Flatten both outputs and targets\n",
        "        outputs_flat = outputs.view(-1, outputs.shape[-1])\n",
        "        target_ids_flat = target_ids.view(-1)\n",
        "\n",
        "        loss = loss_fn(outputs_flat, target_ids_flat)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "end_time = time.time()\n"
      ],
      "metadata": {
        "id": "VU8UJH2vWgNa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elapsed_time = end_time - start_time\n",
        "print(f\"batch took {elapsed_time:.6f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFxloF6BSyRc",
        "outputId": "3f74e4bd-f755-48f6-b2e8-d52661a00c77"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch took 147.466052 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"fine_tuned_bert_model\")\n",
        "tokenizer.save_pretrained(\"fine_tuned_bert_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj3UVa1dLh2k",
        "outputId": "a200997d-beca-4cf4-e00d-392cb46700b5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fine_tuned_bert_model/tokenizer_config.json',\n",
              " 'fine_tuned_bert_model/special_tokens_map.json',\n",
              " 'fine_tuned_bert_model/vocab.txt',\n",
              " 'fine_tuned_bert_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model\n",
        "fine_tuned_model = BertForMaskedLM.from_pretrained(\"fine_tuned_bert_model\")\n",
        "\n"
      ],
      "metadata": {
        "id": "wGwuF85jWjiK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Inference\n",
        "# input_text = \"मैं\"\n",
        "# input_ids = tokenizer.encode(input_text, add_special_tokens=True)\n",
        "# with torch.no_grad():\n",
        "#     outputs = fine_tuned_model(torch.tensor(input_ids).unsqueeze(0))\n",
        "#     predicted_token_id = torch.argmax(outputs[0, -1]).item()\n",
        "#     predicted_word = tokenizer.convert_ids_to_tokens(predicted_token_id)\n",
        "\n",
        "# print(\"Predicted next word:\", predicted_word)\n",
        "\n",
        "# print(outputs.logits)\n",
        "\n",
        "# Input text\n",
        "input_text = \"मैं\"\n",
        "\n",
        "# Encode input text and perform inference\n",
        "input_ids = tokenizer.encode(input_text, add_special_tokens=True)\n",
        "with torch.no_grad():\n",
        "    outputs = fine_tuned_model(torch.tensor(input_ids).unsqueeze(0))\n",
        "    print(outputs.logits)\n",
        "    predicted_token_id = torch.argmax(outputs.logits[0, -1]).item()\n",
        "    predicted_word = tokenizer.convert_ids_to_tokens(predicted_token_id)\n",
        "\n",
        "print(\"Predicted word:\", predicted_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xZE3tGmWkqs",
        "outputId": "38c280d2-2beb-4353-f89b-5ac8b1815fd5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-6.7139, -7.6789, -6.9048,  ..., -6.3974, -6.1907, -6.0965],\n",
            "         [-6.9662, -7.6907, -7.6814,  ..., -6.7100, -5.8716, -6.6599],\n",
            "         [-5.7604, -7.0342, -6.8200,  ..., -6.3531, -5.1109, -5.8730],\n",
            "         [-6.7318, -7.6566, -7.1837,  ..., -6.7691, -5.6489, -7.0067]]])\n",
            "Predicted word: स\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "input_text = \"मैं\"\n",
        "\n",
        "# Encode input text\n",
        "input_ids = tokenizer.encode(input_text, add_special_tokens=True)\n",
        "\n",
        "generated_ids = input_ids.copy()\n",
        "with torch.no_grad():\n",
        "    for _ in range(50):\n",
        "        input_tensor = torch.tensor(generated_ids).unsqueeze(0)\n",
        "        outputs = fine_tuned_model(input_tensor)\n",
        "        predicted_token_id = torch.argmax(outputs.logits[0, -1]).item()\n",
        "        generated_ids.append(predicted_token_id)\n",
        "        if predicted_token_id == tokenizer.sep_token_id:\n",
        "            break\n",
        "\n",
        "# Convert the generated IDs to words\n",
        "generated_words = tokenizer.convert_ids_to_tokens(generated_ids)\n",
        "\n",
        "# Print the generated sequence\n",
        "generated_sequence = \" \".join(generated_words)\n",
        "print(\"Generated sequence:\", generated_sequence)\n",
        "tokenizer.convert_tokens_to_string(generated_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "YpApvRykMLWw",
        "outputId": "ea2e768f-6803-4216-e1ea-64d6b3bec1a0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sequence: [CLS] म ##ैं [SEP] स ##ु ##म ##झ ##े की त ##हत ##ा ##उन ##ल , स ##ु ##ध ##्या पर न ##ाग ##रिक ##रिक ##ता है । ये त ##हत ##ा ##उन ##ल में म ##ौ ##जूद एस ##ई दे ##खने के लिए केंद्र ##ीय ##ीय सरकार व ##र ##न ##ून ##ून ##ी\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] मैं [SEP] सुमझे की तहताउनल , सुध्या पर नागरिकरिकता है । ये तहताउनल में मौजूद एसई देखने के लिए केंद्रीयीय सरकार वरनूनूनी'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the tokenizer for a multilingual BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "# Hindi input text\n",
        "hindi_text = \"मैंने खाना खाया\"\n",
        "\n",
        "# Tokenize the Hindi text\n",
        "tokens = tokenizer.tokenize(hindi_text)\n",
        "print(tokens)\n",
        "\n",
        "# Convert tokens to IDs\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(input_ids)\n"
      ],
      "metadata": {
        "id": "cL0XvEqmP1XN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd0fc1f-3670-4f22-edc9-920e0f97dbff"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['म', '##ैं', '##ने', 'खान', '##ा', 'खा', '##या']\n",
            "[889, 99007, 13466, 101415, 11208, 64566, 15168]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the tokenizer for a multilingual BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "# List of token IDs for Hindi words\n",
        "hindi_word_ids = [tokenizer.convert_tokens_to_ids(token) for token in ['म', '##ैं', '##ने', 'खान', '##ा', 'खा', '##या']]\n",
        "\n",
        "# Convert token IDs back to words\n",
        "hindi_words = tokenizer.convert_ids_to_tokens(hindi_word_ids)\n",
        "print(hindi_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sjFCzI-VPuY",
        "outputId": "fee55727-55de-4d5f-9da5-627665ff07fd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['म', '##ैं', '##ने', 'खान', '##ा', 'खा', '##या']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_tokens_to_string(['म', '##ैं', '##ने', 'खान', '##ा', 'खा', '##या'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VjGed4H5WKo7",
        "outputId": "8bb66142-0ae6-48c7-ee4c-4239fa601c84"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'मैंने खाना खाया'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pxkGbkIEWX7K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}